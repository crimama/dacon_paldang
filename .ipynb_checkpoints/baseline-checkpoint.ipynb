{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af976f7f-285d-4747-97cb-b7ef4b5d00f4",
   "metadata": {},
   "source": [
    "# 데이터 로드 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3287506-d5b2-4398-afc5-9fb9f101c429",
   "metadata": {},
   "source": [
    "Data\n",
    "├ Water Data\n",
    "\n",
    "│ ├ data_2012.csv\n",
    "\n",
    "│   ├ ymdhm : 년월일시분\n",
    "\n",
    "│   ├ swl : 팔당댐 현재수위 (단위: El.m)\n",
    "\n",
    "│   ├ inf : 팔당댐 유입량 (단위: m^3/s)\n",
    "\n",
    "│   ├ sfw : 팔당댐 저수량 (단위: 만m^3)\n",
    "\n",
    "│   ├ ecpc : 팔당댐 공용량 (단위: 백만m^3)\n",
    "\n",
    "│   ├ tototf : 총 방류량 (단위: m^3/s)\n",
    "\n",
    "│   ├ tide_level : 강화대교 조위 (단위: cm)\n",
    "\n",
    "│   ├ wl_1018662 : 청담대교 수위 (단위: cm)\n",
    "\n",
    "│   ├ fw_1018662 : 청담대교 유량 (단위: m^3/s)\n",
    "\n",
    "│   ├ wl_1018680 : 잠수교 수위 (단위: cm)\n",
    "\n",
    "│   ├ fw_1018680 : 잠수교 유량 (단위: m^3/s)\n",
    "\n",
    "│   ├ wl_1018683 : 한강대교 수위 (단위: cm)\n",
    "\n",
    "│   ├ fw_1018683 : 한강대교 유량 (단위: m^3/s)\n",
    "\n",
    "│   ├ wl_1019630 : 행주대교 수위 (단위: cm)\n",
    "\n",
    "│   └ fw_1019630 : 행주대교 유량 (단위: m^3/s)\n",
    "\n",
    "│ ├ data_2013.csv\n",
    "\n",
    "…\n",
    "\n",
    "└ └ data_2022.csv\n",
    "\n",
    "└ RainFall Data\n",
    "\n",
    "│ ├ rf_2012.csv\n",
    "\n",
    "│   ├ YMDHM : 년월일시분\n",
    "\n",
    "│   ├ rf_10184100 : 대곡교 강수량\n",
    "\n",
    "│   ├ rf_10184110 : 진관교 강수량\n",
    "\n",
    "│   └ rf_10184140 : 송정동 강수량\n",
    "\n",
    "│ ├ rf_2013.csv\n",
    "\n",
    "…\n",
    "\n",
    "└ └ rf_2022.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "922136cc-c294-43dd-95b1-33a3f2a22903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/Dacon_paldang\n"
     ]
    }
   ],
   "source": [
    "%cd Dacon_paldang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e101b60-b464-4363-9b06-43a0041e62a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from scipy import interpolate\n",
    "from DataGenerator import DataGenerator\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU, AveragePooling1D, GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "705329fa-eeed-4d94-b46d-0231cc6d0c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(fold_dir):\n",
    "    file_dirs = sorted(glob(f'{fold_dir}/*'))\n",
    "    df = pd.read_csv(file_dirs[0])\n",
    "    for i in range(1,len(file_dirs)):\n",
    "        temp_df = pd.read_csv(file_dirs[i])\n",
    "        df = pd.concat([df,temp_df])\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df \n",
    "\n",
    "def preprocess_df(df):\n",
    "    #컬럼 설정 \n",
    "    #제외 컬럼 : 'fw_1018680'\n",
    "    columns = ['ymdhm','swl','inf','sfw','ecpc','tototf','tide_level','fw_1018662','fw_1018683','rf_10184100','rf_10184110','rf_10184140','wl_1018662','wl_1018680','wl_1018683','wl_1019630']\n",
    "    df = df[columns]\n",
    "    #결측치 설정 \n",
    "    df = df.fillna(method=\"ffill\")\n",
    "    #데이트타임 \n",
    "    df['ymdhm'] = pd.to_datetime(df['ymdhm'])\n",
    "    #test 데이터 범위 제외 \n",
    "    df = df.iloc[:-6912]\n",
    "    return df \n",
    "    \n",
    "\n",
    "def load_df():\n",
    "    water = load_files('water_data')\n",
    "    rf = load_files('rf_data')\n",
    "    \n",
    "    df = pd.concat([water,rf.drop(columns='ymdhm')],axis=1)\n",
    "    df = preprocess_df(df)\n",
    "    return df \n",
    "\n",
    "def scailng_df(df):\n",
    "    timestamps = df['ymdhm']\n",
    "    min_value = np.min(df,axis=0)\n",
    "    max_value = np.max(df,axis=0)\n",
    "    \n",
    "    scalied_df = (df-min_value)/(max_value-min_value)\n",
    "    \n",
    "    scalied_df['ymdhm'] = timestamps\n",
    "    scaling_value = {'min' : min_value,\n",
    "                     'max' : max_value\n",
    "                    }\n",
    "    \n",
    "    return scalied_df,scaling_value \n",
    "\n",
    "#df 생성 및 전처리 \n",
    "os.chdir('/data/Dacon_paldang/data')\n",
    "df = load_df()\n",
    "df,scaling_value = scailng_df(df)\n",
    "\n",
    "#키값(타임스탬프) 생성 \n",
    "timestamps = df['ymdhm'].to_numpy()[30:]\n",
    "\n",
    "#input - output column \n",
    "input_columns = ['swl','inf','sfw','ecpc','tototf','tide_level','fw_1018662','fw_1018683','wl_1019630','rf_10184100','rf_10184110','rf_10184140']\n",
    "output_columns = ['wl_1018662','wl_1018680','wl_1018683','wl_1019630']\n",
    "\n",
    "#하이퍼 파라미터 \n",
    "opt = {} \n",
    "opt['batch_size'] = 32\n",
    "opt['shuffle'] = True\n",
    "opt['subset_length'] = 6\n",
    "opt['timestamps'] = timestamps \n",
    "opt['input'] = input_columns\n",
    "opt['output_columns'] = output_columns\n",
    "opt['input_columns'] = input_columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578744a-7df0-42b9-bf6a-94f8ec388a29",
   "metadata": {},
   "source": [
    "# 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5fb6d41-917a-4495-ae1c-e8a5afb16448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "tf.executing_eagerly()\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, BatchNormalization,Input,Dropout,Dense,Conv2D,Concatenate,Flatten,Conv1D,Conv2D,MaxPooling1D\n",
    "from tensorflow.keras.layers import Reshape,Embedding,concatenate\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.layers.pooling import GlobalAveragePooling2D,GlobalAveragePooling3D,AveragePooling3D,MaxPooling2D\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97908adc-9e96-49ae-991d-4752722160ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from https://gist.github.com/stared/dfb4dfaf6d9a8501cd1cc8b8cb806d2e\n",
    "# loss 실시간 출력 \n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "n=0\n",
    "class PlotLosses(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.epochs = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.logs = []\n",
    "        self.fig = plt.figure()\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        global n,airport\n",
    "\n",
    "        self.epochs.append(epoch)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.epochs, self.losses, label=\"loss\")\n",
    "        plt.plot(self.epochs, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        print(\"loss = \", self.losses[-1], \", val_loss = \", self.val_losses[-1])\n",
    "        print(f' 현 회차 : {n}_{airport}')\n",
    "\n",
    "#call backs 선언\n",
    "plot_losses = PlotLosses()\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint #<- model 저장 객체, best_model.h5라는 모델 이 저장 됨 \n",
    "model_check_point = ModelCheckpoint(\n",
    "    f'../save_model/',\n",
    "    monitor='val_loss', \n",
    "    verbose=1, \n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "callbacks = [ plot_losses,model_check_point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a641d065-91ab-40b7-90bc-04554b748cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "    # Feed Forward Part\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res \n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(4, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "scheduler = optimizers.schedules.CosineDecay(initial_learning_rate=0.0005,decay_steps=1000,alpha=0.9)\n",
    "    \n",
    "optimizer = optimizers.Adam(learning_rate=scheduler)\n",
    "\n",
    "input_shape = (opt['subset_length'],12)\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=root_mean_squared_error,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"mse\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab7dd2-70af-4745-aed5-9492d34fbd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  73/2105 [>.............................] - ETA: 1:34:12 - loss: 0.3203 - mse: 0.1099"
     ]
    }
   ],
   "source": [
    "opt['batch_size'] = 128\n",
    "\n",
    "train_generator= DataGenerator(opt,df)\n",
    "model.fit(train_generator,epochs=20,verbose=1,callbacks = callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
